# Experiment configurations

experiments:
  # Split degeneration experiment by model capabilities
  degeneration_local:
    description: "Test repetition rates for local models with all methods (Holtzman et al. 2019)"
    models:
      - gpt2           # Fast baseline
      - gpt2-large     # Verified working
      - qwen2.5-7b     # Modern 7B model
      - qwen2.5-72b    # Large model verified working
      - mixtral-8x7b   # MoE architecture verified working
      - mistral-7b     # Awaiting access
      - mistral-small-3-24b  # Awaiting access
      - llama3-70b     # Awaiting access
    methods:
      - greedy
      - beam_5
      - beam_10
      - nucleus_0.95
      - nucleus_0.9
      - top_k_50
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  degeneration_openai:
    description: "Test repetition rates for OpenAI models (limited methods)"
    models:
      - gpt-4           # Verified working
      - gpt-5           # Will be available soon
    methods:
      - greedy
      - nucleus_0.95
      - nucleus_0.9
      # No beam search, no top_k simulation due to limitations
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  degeneration_anthropic:
    description: "Test repetition rates for Anthropic models (very limited methods)"
    models:
      - claude-3-5-sonnet-20241022  # Verified working
      - claude-4-opus               # Will be available soon
    methods:
      - greedy
      - nucleus_0.95
      - nucleus_0.9
      # No beam search, no top_k support
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  # Only test perplexity on models that support it
  perplexity_local:
    description: "Test perplexity gap for local models (full logprobs)"
    models:
      - gpt2           # Fast baseline
      - gpt2-large     # Verified working
      - qwen2.5-7b     # Modern 7B model
      - qwen2.5-72b    # Large model verified working
      - mixtral-8x7b   # MoE architecture verified working
      - mistral-7b     # Awaiting access
      - mistral-small-3-24b  # Awaiting access
      - llama3-70b     # Awaiting access
    methods:
      - beam_10
      - nucleus_0.95
    num_samples: 100  # Reduced for testing
    max_length: 256

  perplexity_openai:
    description: "Test perplexity gap for OpenAI (approximate, top-5 only)"
    models:
      - gpt-4
    methods:
      - nucleus_0.95  # Skip beam search
    num_samples: 50   # Reduced due to API costs and limitations
    max_length: 256

  # Only test tail analysis on models that support full distributions
  tail_analysis:
    description: "Test reliability of low-probability tokens (local models only)"
    models:
      - gpt2           # Fast baseline
      - gpt2-large     # Verified working
      - qwen2.5-7b     # Modern 7B model
      - qwen2.5-72b    # Large model verified working
      - mixtral-8x7b   # MoE architecture verified working
      - mistral-7b     # Awaiting access
      - mistral-small-3-24b  # Awaiting access
      - llama3-70b     # Awaiting access
    probability_ranges:
      head: [0.0, 0.1]     # Top 10% probability mass
      middle: [0.1, 0.5]   # 10-50% probability mass
      tail: [0.95, 1.0]    # Bottom 5% probability mass
    samples_per_range: 20
    max_length: 100

  # Task-specific experiments (commented out for now)
  # TODO: Need to create proper creative and factual prompt files
  # - Creative: Story starters, character descriptions, creative scenarios
  # - Factual: Knowledge questions, fact completion, QA pairs
  # - Code: Function signatures, algorithmic problems, debugging tasks
  # task_specific_api:
  #   description: "Test optimal methods for different task types (API models)"
  #   models:
  #     - gpt-4
  #     - claude-3-5-sonnet-20241022
  #   methods:
  #     - greedy
  #     - nucleus_0.95
  #     # Removed beam_10 - not supported by API models
  #   tasks:
  #     creative:
  #       prompts_file: "prompts/creative.txt"  # TODO: Create this file
  #       num_samples: 20  # Reduced for cost
  #     factual:
  #       prompts_file: "prompts/factual.txt"   # TODO: Create this file
  #       num_samples: 20
  #     code:
  #       prompts_file: "prompts/code.txt"      # TODO: Create this file
  #       num_samples: 20

  beam_curse:
    description: "Test if quality decreases with larger beam sizes"
    models:
      # Only open source models support beam search
      - gpt2           # Fast baseline
      - gpt2-large     # Verified working
      - qwen2.5-7b     # Modern 7B model
      - qwen2.5-72b    # Large model verified working
      - mixtral-8x7b   # MoE architecture verified working
      - mistral-7b     # Awaiting access
      - mistral-small-3-24b  # Awaiting access
      - llama3-70b     # Awaiting access
    beam_sizes: [1, 2, 5, 10, 20, 50]
    num_samples: 100
    max_length: 256

# Default parameters for all experiments
defaults:
  temperature: 1.0
  top_p: 0.95
  top_k: 50
  seed: 42