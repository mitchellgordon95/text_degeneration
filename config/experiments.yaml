# Experiment configurations

experiments:
  # Split degeneration experiment by model capabilities
  degeneration_local:
    description: "Test repetition rates for local models with all methods (Holtzman et al. 2019)"
    models:
      - gpt2           # Fast baseline
      - gpt2-large     # Verified working
      - qwen2.5-7b     # Modern 7B model
      - qwen2.5-72b    # Large model verified working
      - mixtral-8x7b   # MoE architecture verified working
      - mistral-7b     # Available
      - mistral-small-3-24b  # Available
      - llama3-70b     # Available
    methods:
      - greedy
      - beam_16        # Match Holtzman's beam size
      - beam_10        # Additional comparison
      - pure_sampling  # Temperature=1.0, no truncation
      - top_k_40       # Match Holtzman's k value
      - nucleus_0.95   # Holtzman's recommended
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  degeneration_openai:
    description: "Test repetition rates for OpenAI models (limited methods)"
    models:
      - gpt-4           # Verified working
      - gpt-5           # Available
    methods:
      - greedy
      - nucleus_0.95
      - nucleus_0.9
      # No beam search, no top_k simulation due to limitations
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  degeneration_anthropic:
    description: "Test repetition rates for Anthropic models (very limited methods)"
    models:
      - claude-3-5-sonnet-20241022  # Verified working
      - claude-4-opus               # Available
    methods:
      - greedy
      - nucleus_0.95
      - nucleus_0.9
      # No beam search, no top_k support
    num_samples: 200
    max_length: 256
    n_gram_size: 4

  # Note: Perplexity gap is now computed as part of degeneration experiments
  # No need for separate perplexity experiments

  # Future experiments (not yet implemented)
  # tail_analysis:
  #   description: "Test reliability of low-probability tokens (local models only)"
  #   models: [gpt2, gpt2-large, qwen2.5-7b, qwen2.5-72b, mixtral-8x7b]
  #   probability_ranges:
  #     head: [0.0, 0.1]     # Top 10% probability mass
  #     middle: [0.1, 0.5]   # 10-50% probability mass
  #     tail: [0.95, 1.0]    # Bottom 5% probability mass
  #   samples_per_range: 20
  #   max_length: 100

  # Task-specific experiments (commented out for now)
  # TODO: Need to create proper creative and factual prompt files
  # - Creative: Story starters, character descriptions, creative scenarios
  # - Factual: Knowledge questions, fact completion, QA pairs
  # - Code: Function signatures, algorithmic problems, debugging tasks
  # task_specific_api:
  #   description: "Test optimal methods for different task types (API models)"
  #   models:
  #     - gpt-4
  #     - claude-3-5-sonnet-20241022
  #   methods:
  #     - greedy
  #     - nucleus_0.95
  #     # Removed beam_10 - not supported by API models
  #   tasks:
  #     creative:
  #       prompts_file: "prompts/creative.txt"  # TODO: Create this file
  #       num_samples: 20  # Reduced for cost
  #     factual:
  #       prompts_file: "prompts/factual.txt"   # TODO: Create this file
  #       num_samples: 20
  #     code:
  #       prompts_file: "prompts/code.txt"      # TODO: Create this file
  #       num_samples: 20

  # beam_curse:
  #   description: "Test if quality decreases with larger beam sizes"
  #   models: [gpt2, gpt2-large, qwen2.5-7b, qwen2.5-72b, mixtral-8x7b]
  #   beam_sizes: [1, 2, 5, 10, 20, 50]
  #   num_samples: 100
  #   max_length: 256

# Default parameters for all experiments
defaults:
  temperature: 1.0
  top_p: 0.95
  top_k: 50
  seed: 42