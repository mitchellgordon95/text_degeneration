# Model configurations

models:
  # GPT-2 models (HuggingFace)
  gpt2:
    type: vllm
    model_id: gpt2
    device: auto
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - autoregressive causal LM"

  gpt2-medium:
    type: vllm
    model_id: gpt2-medium
    device: auto
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - autoregressive causal LM"

  gpt2-large:
    type: vllm
    model_id: gpt2-large
    device: auto
    load_in_8bit: false  # Set to true if low on VRAM
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - autoregressive causal LM, used in Holtzman et al. 2019"

  gpt2-xl:
    type: vllm
    model_id: gpt2-xl
    device: auto
    load_in_8bit: false
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - autoregressive causal LM"

  # OpenAI models
  gpt-3.5-turbo-instruct:
    type: openai
    model_id: gpt-3.5-turbo-instruct
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: true
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "top_5_logprobs_only", "no_top_k_native", "no_full_vocab_access"]
      notes: "OpenAI API - no beam search, limited logprobs (top-5 only)"

  gpt-4:
    type: openai
    model_id: gpt-4
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: true
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "top_5_logprobs_only", "no_top_k_native", "no_full_vocab_access"]
      notes: "OpenAI Chat API - supports logprobs (top-5), no beam search"

  gpt-4-turbo:
    type: openai
    model_id: gpt-4-turbo-preview
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: true
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "top_5_logprobs_only", "no_top_k_native", "no_full_vocab_access"]
      notes: "OpenAI API - no beam search, limited logprobs (top-5 only)"

  gpt-5:
    type: openai
    model_id: gpt-5
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: true
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "top_5_logprobs_only", "no_top_k_native", "no_full_vocab_access"]
      notes: "OpenAI API - no beam search, limited logprobs (top-5 only)"

  # Anthropic models
  claude-3-5-sonnet-20241022:
    type: anthropic
    model_id: claude-3-5-sonnet-20241022
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: false
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "no_logprobs", "no_top_k_native", "no_perplexity", "no_vocab_access"]
      notes: "Anthropic API - no logprobs, no beam search, very limited"

  claude-3-opus:
    type: anthropic
    model_id: claude-3-opus-20240229
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: false
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "no_logprobs", "no_top_k_native", "no_perplexity", "no_vocab_access"]
      notes: "Anthropic API - no logprobs, no beam search, very limited"

  claude-3-haiku:
    type: anthropic
    model_id: claude-3-haiku-20240307
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: false
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "no_logprobs", "no_top_k_native", "no_perplexity", "no_vocab_access"]
      notes: "Anthropic API - no logprobs, no beam search, very limited"

  claude-4-opus:
    type: anthropic
    model_id: claude-opus-4-1-20250805
    capabilities:
      supported_methods: ["greedy", "temperature", "nucleus"]
      supports_logprobs: false
      supports_full_logprobs: false
      supports_beam_search: false
      max_beam_size: null
      limitations: ["no_beam_search", "no_logprobs", "no_top_k_native", "no_perplexity", "no_vocab_access"]
      notes: "Anthropic API - no logprobs, no beam search, very limited"

  # Llama models (HuggingFace)
  llama3-8b:
    type: vllm
    model_id: meta-llama/Meta-Llama-3-8B-Instruct
    device: auto
    load_in_8bit: true  # Recommended for consumer GPUs
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Llama-3 8B instruction-tuned model"

  llama3-70b:
    type: vllm
    model_id: meta-llama/Llama-3.3-70B-Instruct  # December 2024 release
    device: auto
    quantization: "fp8"           # FP8 dynamic quantization (no pre-quantized model needed)
    gpu_memory_utilization: 0.90  # Increase memory utilization
    max_model_len: 2048           # Further reduce context length
    cpu_offload_gb: 8             # Increase CPU offloading
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Llama-3.3 70B instruction-tuned model"

  # Qwen models (Alibaba) - September 2024 release
  qwen2.5-7b:
    type: vllm
    model_id: Qwen/Qwen2.5-7B-Instruct
    device: auto
    load_in_8bit: false
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Qwen2.5 7B instruction-tuned model"

  qwen2.5-72b:
    type: vllm
    model_id: Qwen/Qwen2.5-72B-Instruct  # September 2024 release
    device: auto
    quantization: "fp8"          # FP8 quantization optimized for H100 GPUs
    gpu_memory_utilization: 0.85  # Balance between model and KV cache
    max_model_len: 4096           # Reduce context length to fit in memory
    cpu_offload_gb: 4             # Offload some layers to CPU if needed
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Qwen2.5 72B instruction-tuned model"

  # Mistral models
  mistral-7b:
    type: vllm
    model_id: mistralai/Mistral-7B-Instruct-v0.3
    device: auto
    load_in_8bit: false
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Mistral 7B instruction-tuned model"

  mistral-small-3-24b:
    type: vllm
    model_id: mistralai/Mistral-Small-24B-Instruct-2501  # January 2025 release
    device: auto
    load_in_8bit: true
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Mistral Small 24B instruction-tuned model"

  mixtral-8x7b:
    type: vllm
    model_id: mistralai/Mixtral-8x7B-Instruct-v0.1  # MoE architecture
    device: auto
    quantization: "fp8"          # FP8 quantization optimized for H100 GPUs
    gpu_memory_utilization: 0.85  # MoE is more memory efficient, slightly higher
    max_model_len: 8192           # MoE can handle longer contexts
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - Mixtral 8x7B MoE instruction-tuned model"

  # DeepSeek models
  deepseek-7b:
    type: vllm
    model_id: deepseek-ai/deepseek-llm-7b-chat
    device: auto
    load_in_8bit: false
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - DeepSeek 7B chat model"

  deepseek-67b:
    type: vllm
    model_id: deepseek-ai/deepseek-llm-67b-chat
    device: auto
    quantization: "fp8"          # FP8 quantization optimized for H100 GPUs
    gpu_memory_utilization: 0.85  # Balance between model and KV cache
    max_model_len: 4096           # Reduce context length to fit in memory
    cpu_offload_gb: 4             # Offload some layers to CPU if needed
    capabilities:
      supported_methods: ["greedy", "beam", "temperature", "nucleus", "top_k", "pure_sampling"]
      supports_logprobs: true
      supports_full_logprobs: true
      supports_beam_search: true
      max_beam_size: 50
      limitations: []
      notes: "Full capabilities - DeepSeek 67B chat model"
